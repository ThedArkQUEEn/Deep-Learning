{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c9d92b",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f1fecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 20:56:55.593704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750427815.675718  210720 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750427815.689515  210720 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-20 20:56:55.733984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version: 2.18.0\n",
      "Found 1 GPU(s), memory growth enabled\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "loss_fn = MeanSquaredError()\n",
    "\n",
    "# --- 1.1. Initial Configuration ---\n",
    "# Ensure TensorFlow version is 2.0 or higher\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "print(f\"Using TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Set seeds for NumPy and TensorFlow for reproducible results\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Modern GPU configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Found {len(gpus)} GPU(s), memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a91c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1.2. Function to Create Time Series Dataset ---\n",
    "def generate_time_series(batch_size, n_steps, dtype=tf.float32):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series data consisting of two sine waves with noise.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of time series to generate\n",
    "        n_steps: Number of time steps per series\n",
    "        dtype: Data type for the output tensor\n",
    "    \n",
    "    Returns:\n",
    "        tf.Tensor: Time series data of shape (batch_size, n_steps, 1)\n",
    "    \"\"\"\n",
    "    freq1, freq2 = np.random.rand(2, batch_size, 1)\n",
    "    offsets1, offsets2 = np.random.rand(2, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    \n",
    "    # Create two sine wave components\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)  # Add noise\n",
    "    \n",
    "    return tf.constant(series[..., np.newaxis], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e76723b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating time series with 50 time steps...\n",
      "Training data shape (X, y): (7000, 50, 1), (7000, 1)\n",
      "Validation data shape (X, y): (2000, 50, 1), (2000, 1)\n",
      "Test data shape (X, y): (1000, 50, 1), (1000, 1)\n",
      "Data setup complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750427822.262467  210720 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# --- 1.3. Create and Split Dataset ---\n",
    "n_steps = 50\n",
    "print(f\"Generating time series with {n_steps} time steps...\")\n",
    "\n",
    "# Generate main dataset\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
    "\n",
    "print(f\"Training data shape (X, y): {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation data shape (X, y): {X_valid.shape}, {y_valid.shape}\")\n",
    "print(f\"Test data shape (X, y): {X_test.shape}, {y_test.shape}\")\n",
    "print(\"Data setup complete.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ff111",
   "metadata": {},
   "source": [
    "### 2. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69801522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1750427822.423177  210720 gpu_backend_lib.cc:579] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  ipykernel_launcher.runfiles/cuda_nvcc\n",
      "  ipykern/cuda_nvcc\n",
      "  \n",
      "  /usr/local/cuda\n",
      "  /home/ardi/miniconda3/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/ardi/miniconda3/lib/python3.12/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  /home/ardi/miniconda3/lib/python3.12/site-packages/tensorflow/python/platform/../../cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n"
     ]
    }
   ],
   "source": [
    "# --- 2.1. Naive Prediction ---\n",
    "y_pred_naive = X_valid[:, -1, 0]  # Use last observed value\n",
    "naive_mse = loss_fn(y_valid[:, 0], y_pred_naive)\n",
    "naive_mse = tf.reduce_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37a93bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardi/miniconda3/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training simple linear model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750427823.876846  210792 service.cc:148] XLA service 0x7f5c10005310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1750427823.876895  210792 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 2060, Compute Capability 7.5\n",
      "2025-06-20 20:57:03.893280: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1750427823.936496  210792 cuda_dnn.cc:529] Loaded cuDNN version 91000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 33/219\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4471 - mae: 0.5414  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750427824.162046  210792 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.2780 - mae: 0.4165 - val_loss: 0.0543 - val_mae: 0.1881\n",
      "Epoch 2/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0449 - mae: 0.1724 - val_loss: 0.0263 - val_mae: 0.1330\n",
      "Epoch 3/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0240 - mae: 0.1267 - val_loss: 0.0187 - val_mae: 0.1126\n",
      "Epoch 4/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0174 - mae: 0.1085 - val_loss: 0.0152 - val_mae: 0.1008\n",
      "Epoch 5/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0142 - mae: 0.0974 - val_loss: 0.0131 - val_mae: 0.0928\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0132 - mae: 0.0933\n",
      "Baseline - Linear Model MSE: 0.0131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2.2. Simple Linear Model ---\n",
    "linear_model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[n_steps, 1]),\n",
    "    layers.Dense(1, name='output')\n",
    "], name='linear_model')\n",
    "\n",
    "linear_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"Training simple linear model...\")\n",
    "linear_history = linear_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "linear_mse = linear_model.evaluate(X_valid, y_valid, verbose=1)[0]\n",
    "print(f\"Baseline - Linear Model MSE: {linear_mse:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0f382",
   "metadata": {},
   "source": [
    "### 3. Simple Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b42664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training simple RNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardi/miniconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - loss: 0.0239 - mae: 0.0973 - val_loss: 0.0038 - val_mae: 0.0489\n",
      "Epoch 2/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0038 - mae: 0.0493 - val_loss: 0.0040 - val_mae: 0.0503\n",
      "Epoch 3/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0038 - mae: 0.0496 - val_loss: 0.0038 - val_mae: 0.0488\n",
      "Epoch 4/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0038 - mae: 0.0496 - val_loss: 0.0038 - val_mae: 0.0491\n",
      "Epoch 5/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0039 - mae: 0.0499 - val_loss: 0.0038 - val_mae: 0.0492\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0036 - mae: 0.0475\n",
      "RNN - Simple RNN MSE: 0.0038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3.1. Model with Single SimpleRNN Layer ---\n",
    "simple_rnn_model = keras.Sequential([\n",
    "    layers.SimpleRNN(20, input_shape=[None, 1], name='simple_rnn'),\n",
    "    layers.Dense(1, name='output')\n",
    "], name='simple_rnn_model')\n",
    "\n",
    "simple_rnn_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"Training simple RNN model...\")\n",
    "simple_rnn_history = simple_rnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "simple_rnn_mse = simple_rnn_model.evaluate(X_valid, y_valid, verbose=1)[0]\n",
    "print(f\"RNN - Simple RNN MSE: {simple_rnn_mse:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021e1cc7",
   "metadata": {},
   "source": [
    "### 4. Deep RNN (Stacked RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b94565dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Deep RNN model...\n",
      "Epoch 1/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - loss: 0.1096 - mae: 0.2433 - val_loss: 0.0044 - val_mae: 0.0529\n",
      "Epoch 2/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0139 - mae: 0.0938 - val_loss: 0.0035 - val_mae: 0.0470\n",
      "Epoch 3/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0093 - mae: 0.0764 - val_loss: 0.0037 - val_mae: 0.0489\n",
      "Epoch 4/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0078 - mae: 0.0704 - val_loss: 0.0036 - val_mae: 0.0481\n",
      "Epoch 5/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 0.0067 - mae: 0.0653 - val_loss: 0.0033 - val_mae: 0.0456\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0032 - mae: 0.0446\n",
      "RNN - Deep RNN MSE: 0.0033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4.1. Deep RNN Model ---\n",
    "deep_rnn_model = keras.Sequential([\n",
    "    layers.SimpleRNN(50, return_sequences=True, input_shape=[None, 1], name='rnn_1'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.SimpleRNN(50, name='rnn_2'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, name='output')\n",
    "], name='deep_rnn_model')\n",
    "\n",
    "deep_rnn_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"Training Deep RNN model...\")\n",
    "deep_rnn_history = deep_rnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "deep_rnn_mse = deep_rnn_model.evaluate(X_valid, y_valid, verbose=1)[0]\n",
    "print(f\"RNN - Deep RNN MSE: {deep_rnn_mse:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869a190",
   "metadata": {},
   "source": [
    "### 5. Multi-Step Forecasting (Sequence-to-Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314cd294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1. Data Preparation for 10-Step Forecasting ---\n",
    "def prepare_multioutput_data(batch_size, n_steps, n_outputs=10):\n",
    "    \"\"\"Prepare data for multi-step forecasting\"\"\"\n",
    "    series_multi = generate_time_series(batch_size, n_steps + n_outputs)\n",
    "    X = series_multi[:, :n_steps]\n",
    "    Y = series_multi[:, -n_outputs:, 0]  # Remove last dimension for Y\n",
    "    return X, Y\n",
    "\n",
    "n_outputs = 10\n",
    "X_train_multi, Y_train_multi = prepare_multioutput_data(7000, n_steps, n_outputs)\n",
    "X_valid_multi, Y_valid_multi = prepare_multioutput_data(2000, n_steps, n_outputs)\n",
    "X_test_multi, Y_test_multi = prepare_multioutput_data(1000, n_steps, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda7e8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sequence-to-Vector model...\n",
      "Epoch 1/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - loss: 0.0778 - mae: 0.2252 - val_loss: 0.0251 - val_mae: 0.1308\n",
      "Epoch 2/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - loss: 0.0286 - mae: 0.1380 - val_loss: 0.0198 - val_mae: 0.1174\n",
      "Epoch 3/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - loss: 0.0241 - mae: 0.1275 - val_loss: 0.0188 - val_mae: 0.1136\n",
      "Epoch 4/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - loss: 0.0223 - mae: 0.1218 - val_loss: 0.0181 - val_mae: 0.1092\n",
      "Epoch 5/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - loss: 0.0205 - mae: 0.1156 - val_loss: 0.0142 - val_mae: 0.0960\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0144 - mae: 0.0967\n",
      "Multi-step - Sequence-to-Vector MSE: 0.0142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5.2. Sequence-to-Vector Model ---\n",
    "seq_to_vec_model = keras.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True, input_shape=[None, 1], name='lstm_1'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.LSTM(64, name='lstm_2'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu', name='dense_1'),\n",
    "    layers.Dense(n_outputs, name='output')\n",
    "], name='seq_to_vec_model')\n",
    "\n",
    "seq_to_vec_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"Training Sequence-to-Vector model...\")\n",
    "seq_to_vec_history = seq_to_vec_model.fit(\n",
    "    X_train_multi, Y_train_multi,\n",
    "    epochs=5,\n",
    "    validation_data=(X_valid_multi, Y_valid_multi),\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "seq_to_vec_mse = seq_to_vec_model.evaluate(X_valid_multi, Y_valid_multi, verbose=1)[0]\n",
    "print(f\"Multi-step - Sequence-to-Vector MSE: {seq_to_vec_mse:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36215279",
   "metadata": {},
   "source": [
    "### 6. Sequence-to-Sequence Forcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2471559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.1. Sequence-to-Sequence Data Preparation ---\n",
    "def prepare_seq2seq_data(X, series_multi, n_steps, n_outputs=10):\n",
    "    \"\"\"Prepare data for sequence-to-sequence forecasting\"\"\"\n",
    "    batch_size = X.shape[0]\n",
    "    Y_seq = np.empty((batch_size, n_steps, n_outputs))\n",
    "    \n",
    "    for step_ahead in range(1, n_outputs + 1):\n",
    "        Y_seq[..., step_ahead - 1] = series_multi[:, step_ahead:step_ahead + n_steps, 0]\n",
    "    \n",
    "    return tf.constant(Y_seq, dtype=tf.float32)\n",
    "\n",
    "# Prepare sequence-to-sequence data\n",
    "series_full = generate_time_series(10000, n_steps + n_outputs)\n",
    "X_train_seq = series_full[:7000, :n_steps]\n",
    "X_valid_seq = series_full[7000:9000, :n_steps]\n",
    "\n",
    "Y_train_seq = prepare_seq2seq_data(X_train_seq, series_full[:7000], n_steps, n_outputs)\n",
    "Y_valid_seq = prepare_seq2seq_data(X_valid_seq, series_full[7000:9000], n_steps, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b28bcfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sequence-to-Sequence model...\n",
      "Epoch 1/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 60ms/step - last_time_step_mse: 0.0726 - loss: 0.0836 - val_last_time_step_mse: 0.0227 - val_loss: 0.0408\n",
      "Epoch 2/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 61ms/step - last_time_step_mse: 0.0262 - loss: 0.0427 - val_last_time_step_mse: 0.0147 - val_loss: 0.0310\n",
      "Epoch 3/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - last_time_step_mse: 0.0185 - loss: 0.0347 - val_last_time_step_mse: 0.0123 - val_loss: 0.0280\n",
      "Epoch 4/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 52ms/step - last_time_step_mse: 0.0157 - loss: 0.0314 - val_last_time_step_mse: 0.0101 - val_loss: 0.0255\n",
      "Epoch 5/5\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 62ms/step - last_time_step_mse: 0.0137 - loss: 0.0294 - val_last_time_step_mse: 0.0090 - val_loss: 0.0244\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - last_time_step_mse: 0.0091 - loss: 0.0244\n",
      "Seq2Seq - Total MSE: 0.0244\n",
      "Seq2Seq - Last Time Step MSE: 0.0090\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 6.2. Sequence-to-Sequence Model ---\n",
    "seq_to_seq_model = keras.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True, input_shape=[None, 1], name='lstm_1'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.LSTM(64, return_sequences=True, name='lstm_2'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.TimeDistributed(layers.Dense(32, activation='relu'), name='dense_distributed'),\n",
    "    layers.TimeDistributed(layers.Dense(n_outputs), name='output_distributed')\n",
    "], name='seq_to_seq_model')\n",
    "\n",
    "# Custom metric for last time step MSE\n",
    "class LastTimeStepMSE(keras.metrics.Metric):\n",
    "    def __init__(self, name='last_time_step_mse', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.mse = self.add_weight(name='mse', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mse = tf.reduce_mean(loss_fn(y_true[:, -1], y_pred[:, -1]))\n",
    "        self.mse.assign_add(mse)\n",
    "        self.count.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return self.mse / self.count\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.mse.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "seq_to_seq_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[LastTimeStepMSE()]\n",
    ")\n",
    "\n",
    "print(\"Training Sequence-to-Sequence model...\")\n",
    "seq_to_seq_history = seq_to_seq_model.fit(\n",
    "    X_train_seq, Y_train_seq,\n",
    "    epochs=5,\n",
    "    validation_data=(X_valid_seq, Y_valid_seq),\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "seq_to_seq_results = seq_to_seq_model.evaluate(X_valid_seq, Y_valid_seq, verbose=1)\n",
    "print(f\"Seq2Seq - Total MSE: {seq_to_seq_results[0]:.4f}\")\n",
    "print(f\"Seq2Seq - Last Time Step MSE: {seq_to_seq_results[1]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd722f",
   "metadata": {},
   "source": [
    "### 7. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d8aa91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training advanced LSTM model...\n",
      "Epoch 1/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 65ms/step - last_time_step_mse: 0.3039 - loss: 0.3085 - val_last_time_step_mse: 0.1112 - val_loss: 0.1153 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 63ms/step - last_time_step_mse: 0.0414 - loss: 0.0561 - val_last_time_step_mse: 0.0587 - val_loss: 0.0713 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 81ms/step - last_time_step_mse: 0.0241 - loss: 0.0391 - val_last_time_step_mse: 0.0276 - val_loss: 0.0413 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 68ms/step - last_time_step_mse: 0.0192 - loss: 0.0334 - val_last_time_step_mse: 0.0156 - val_loss: 0.0290 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 72ms/step - last_time_step_mse: 0.0164 - loss: 0.0302 - val_last_time_step_mse: 0.0117 - val_loss: 0.0260 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 66ms/step - last_time_step_mse: 0.0143 - loss: 0.0281 - val_last_time_step_mse: 0.0094 - val_loss: 0.0229 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 75ms/step - last_time_step_mse: 0.0126 - loss: 0.0263 - val_last_time_step_mse: 0.0078 - val_loss: 0.0209 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 60ms/step - last_time_step_mse: 0.0110 - loss: 0.0244 - val_last_time_step_mse: 0.0068 - val_loss: 0.0196 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 64ms/step - last_time_step_mse: 0.0096 - loss: 0.0227 - val_last_time_step_mse: 0.0064 - val_loss: 0.0186 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 76ms/step - last_time_step_mse: 0.0084 - loss: 0.0213 - val_last_time_step_mse: 0.0069 - val_loss: 0.0179 - learning_rate: 0.0010\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - last_time_step_mse: 0.0071 - loss: 0.0181\n",
      "LSTM - Total MSE: 0.0179\n",
      "LSTM - Last Time Step MSE: 0.0069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 7.1. Advanced LSTM Model ---\n",
    "lstm_model = keras.Sequential([\n",
    "    layers.LSTM(128, return_sequences=True, input_shape=[None, 1], name='lstm_1'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.LSTM(128, return_sequences=True, name='lstm_2'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.TimeDistributed(layers.Dense(64, activation='relu'), name='dense_distributed'),\n",
    "    layers.TimeDistributed(layers.Dense(n_outputs), name='output_distributed')\n",
    "], name='advanced_lstm_model')\n",
    "\n",
    "lstm_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "    metrics=[LastTimeStepMSE()]\n",
    ")\n",
    "\n",
    "print(\"Training advanced LSTM model...\")\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_seq, Y_train_seq,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid_seq, Y_valid_seq),\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "lstm_results = lstm_model.evaluate(X_valid_seq, Y_valid_seq, verbose=1)\n",
    "print(f\"LSTM - Total MSE: {lstm_results[0]:.4f}\")\n",
    "print(f\"LSTM - Last Time Step MSE: {lstm_results[1]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c51449c",
   "metadata": {},
   "source": [
    "### 8. GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb1e3b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training advanced GRU model...\n",
      "Epoch 1/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 79ms/step - last_time_step_mse: 0.3474 - loss: 0.3430 - val_last_time_step_mse: 0.0932 - val_loss: 0.0987 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - last_time_step_mse: 0.0535 - loss: 0.0625 - val_last_time_step_mse: 0.0618 - val_loss: 0.0702 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - last_time_step_mse: 0.0270 - loss: 0.0390 - val_last_time_step_mse: 0.0249 - val_loss: 0.0353 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 73ms/step - last_time_step_mse: 0.0202 - loss: 0.0326 - val_last_time_step_mse: 0.0147 - val_loss: 0.0260 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 61ms/step - last_time_step_mse: 0.0181 - loss: 0.0298 - val_last_time_step_mse: 0.0119 - val_loss: 0.0234 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 73ms/step - last_time_step_mse: 0.0154 - loss: 0.0275 - val_last_time_step_mse: 0.0102 - val_loss: 0.0216 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - last_time_step_mse: 0.0130 - loss: 0.0253 - val_last_time_step_mse: 0.0113 - val_loss: 0.0218 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 64ms/step - last_time_step_mse: 0.0113 - loss: 0.0233 - val_last_time_step_mse: 0.0080 - val_loss: 0.0189 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 73ms/step - last_time_step_mse: 0.0102 - loss: 0.0219 - val_last_time_step_mse: 0.0077 - val_loss: 0.0183 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - last_time_step_mse: 0.0095 - loss: 0.0209 - val_last_time_step_mse: 0.0053 - val_loss: 0.0168 - learning_rate: 0.0010\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - last_time_step_mse: 0.0052 - loss: 0.0171\n",
      "GRU - Total MSE: 0.0168\n",
      "GRU - Last Time Step MSE: 0.0053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 8.1. Advanced GRU Model ---\n",
    "gru_model = keras.Sequential([\n",
    "    layers.GRU(128, return_sequences=True, input_shape=[None, 1], name='gru_1'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.GRU(128, return_sequences=True, name='gru_2'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.TimeDistributed(layers.Dense(64, activation='relu'), name='dense_distributed'),\n",
    "    layers.TimeDistributed(layers.Dense(n_outputs), name='output_distributed')\n",
    "], name='advanced_gru_model')\n",
    "\n",
    "gru_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "    metrics=[LastTimeStepMSE()]\n",
    ")\n",
    "\n",
    "print(\"Training advanced GRU model...\")\n",
    "gru_history = gru_model.fit(\n",
    "    X_train_seq, Y_train_seq,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid_seq, Y_valid_seq),\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "gru_results = gru_model.evaluate(X_valid_seq, Y_valid_seq, verbose=1)\n",
    "print(f\"GRU - Total MSE: {gru_results[0]:.4f}\")\n",
    "print(f\"GRU - Last Time Step MSE: {gru_results[1]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90ed95",
   "metadata": {},
   "source": [
    "### 9. Modern CNN for Sequences (WAVENET-Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca98c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9.1. Advanced WaveNet Model ---\n",
    "def create_wavenet_block(inputs, filters, kernel_size, dilation_rate):\n",
    "    \"\"\"Create a single WaveNet residual block\"\"\"\n",
    "    # Dilated convolution\n",
    "    conv = layers.Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding='causal',\n",
    "        activation='tanh'\n",
    "    )(inputs)\n",
    "    \n",
    "    # Gating mechanism\n",
    "    gate = layers.Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding='causal',\n",
    "        activation='sigmoid'\n",
    "    )(inputs)\n",
    "    \n",
    "    # Element-wise multiplication\n",
    "    gated = layers.Multiply()([conv, gate])\n",
    "    \n",
    "    # Skip connection and residual connection\n",
    "    skip = layers.Conv1D(filters, 1)(gated)\n",
    "    residual = layers.Conv1D(filters, 1)(gated)\n",
    "    \n",
    "    if inputs.shape[-1] != filters:\n",
    "        inputs = layers.Conv1D(filters, 1)(inputs)\n",
    "    \n",
    "    return layers.Add()([inputs, residual]), skip\n",
    "\n",
    "# Build WaveNet model with modern architecture\n",
    "def build_wavenet_model(n_filters=64, n_outputs=10):\n",
    "    inputs = layers.Input(shape=[None, 1])\n",
    "    x = layers.Conv1D(n_filters, 1)(inputs)\n",
    "    \n",
    "    skip_connections = []\n",
    "    \n",
    "    # Stack of dilated convolutions\n",
    "    for dilation_rate in [1, 2, 4, 8, 16, 32] * 2:  # 12 blocks total\n",
    "        x, skip = create_wavenet_block(x, n_filters, 2, dilation_rate)\n",
    "        skip_connections.append(skip)\n",
    "    \n",
    "    # Combine skip connections\n",
    "    skip_sum = layers.Add()(skip_connections)\n",
    "    skip_sum = layers.Activation('relu')(skip_sum)\n",
    "    \n",
    "    # Output layers\n",
    "    output = layers.Conv1D(n_filters, 1, activation='relu')(skip_sum)\n",
    "    output = layers.Dropout(0.3)(output)\n",
    "    output = layers.Conv1D(n_outputs, 1)(output)\n",
    "    \n",
    "    return keras.Model(inputs, output, name='modern_wavenet')\n",
    "\n",
    "wavenet_model = build_wavenet_model()\n",
    "wavenet_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "    metrics=[LastTimeStepMSE()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6d2ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training modern WaveNet model...\n",
      "Epoch 1/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 112ms/step - last_time_step_mse: 0.0570 - loss: 0.0668 - val_last_time_step_mse: 0.0136 - val_loss: 0.0267 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - last_time_step_mse: 0.0210 - loss: 0.0339 - val_last_time_step_mse: 0.0090 - val_loss: 0.0222 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - last_time_step_mse: 0.0159 - loss: 0.0286 - val_last_time_step_mse: 0.0067 - val_loss: 0.0195 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - last_time_step_mse: 0.0137 - loss: 0.0260 - val_last_time_step_mse: 0.0055 - val_loss: 0.0178 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - last_time_step_mse: 0.0123 - loss: 0.0242 - val_last_time_step_mse: 0.0051 - val_loss: 0.0169 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - last_time_step_mse: 0.0117 - loss: 0.0229 - val_last_time_step_mse: 0.0048 - val_loss: 0.0161 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - last_time_step_mse: 0.0111 - loss: 0.0220 - val_last_time_step_mse: 0.0044 - val_loss: 0.0153 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - last_time_step_mse: 0.0103 - loss: 0.0212 - val_last_time_step_mse: 0.0044 - val_loss: 0.0150 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - last_time_step_mse: 0.0102 - loss: 0.0205 - val_last_time_step_mse: 0.0045 - val_loss: 0.0148 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - last_time_step_mse: 0.0098 - loss: 0.0200 - val_last_time_step_mse: 0.0036 - val_loss: 0.0139 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "print(\"Training modern WaveNet model...\")\n",
    "wavenet_history = wavenet_model.fit(\n",
    "    X_train_seq, Y_train_seq,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid_seq, Y_valid_seq),\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f95b4aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - last_time_step_mse: 0.0036 - loss: 0.0140\n",
      "WaveNet - Total MSE: 0.0139\n",
      "WaveNet - Last Time Step MSE: 0.0036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wavenet_results = wavenet_model.evaluate(X_valid_seq, Y_valid_seq, verbose=1)\n",
    "print(f\"WaveNet - Total MSE: {wavenet_results[0]:.4f}\")\n",
    "print(f\"WaveNet - Last Time Step MSE: {wavenet_results[1]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c128dbc",
   "metadata": {},
   "source": [
    "### 10. Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b7fbc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim = {embed_dim} should be divisible by num_heads = {num_heads}\")\n",
    "        \n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        \n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        \n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        \n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, ff_dim, n_outputs):\n",
    "    inputs = layers.Input(shape=(maxlen, vocab_size))\n",
    "\n",
    "    # Positional encoding\n",
    "    positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "    positions = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)(positions)\n",
    "\n",
    "    # Project input to embedding dimension\n",
    "    x = layers.Dense(embed_dim)(inputs)\n",
    "    x = x + positions\n",
    "\n",
    "    # Transformer blocks\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x, training=True)\n",
    "\n",
    "    transformer_block2 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block2(x, training=True)\n",
    "\n",
    "    # Global average pooling and output\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(n_outputs * n_steps)(x)\n",
    "    outputs = layers.Reshape((n_steps, n_outputs))(outputs)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name='transformer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81a18646",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = build_transformer_model(\n",
    "    maxlen=n_steps, \n",
    "    vocab_size=1, \n",
    "    embed_dim=64, \n",
    "    num_heads=8, \n",
    "    ff_dim=128, \n",
    "    n_outputs=n_outputs\n",
    ")\n",
    "\n",
    "transformer_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[LastTimeStepMSE()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e99bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer model...\n",
      "Epoch 1/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 66ms/step - last_time_step_mse: 0.1433 - loss: 0.1500 - val_last_time_step_mse: 0.1054 - val_loss: 0.1230 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - last_time_step_mse: 0.1034 - loss: 0.1039 - val_last_time_step_mse: 0.0856 - val_loss: 0.0469 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - last_time_step_mse: 0.0880 - loss: 0.0516 - val_last_time_step_mse: 0.0674 - val_loss: 0.0328 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - last_time_step_mse: 0.0739 - loss: 0.0407 - val_last_time_step_mse: 0.0628 - val_loss: 0.0301 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - last_time_step_mse: 0.0686 - loss: 0.0373 - val_last_time_step_mse: 0.0599 - val_loss: 0.0287 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - last_time_step_mse: 0.0651 - loss: 0.0351 - val_last_time_step_mse: 0.0577 - val_loss: 0.0274 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - last_time_step_mse: 0.0632 - loss: 0.0334 - val_last_time_step_mse: 0.0561 - val_loss: 0.0263 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - last_time_step_mse: 0.0615 - loss: 0.0319 - val_last_time_step_mse: 0.0549 - val_loss: 0.0250 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - last_time_step_mse: 0.0595 - loss: 0.0304 - val_last_time_step_mse: 0.0539 - val_loss: 0.0239 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - last_time_step_mse: 0.0585 - loss: 0.0292 - val_last_time_step_mse: 0.0527 - val_loss: 0.0227 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Transformer model...\")\n",
    "transformer_history = transformer_model.fit(\n",
    "    X_train_seq, Y_train_seq,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid_seq, Y_valid_seq),\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=7, min_lr=1e-7)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff651c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - last_time_step_mse: 0.0528 - loss: 0.0227\n",
      "Transformer - Total MSE: 0.0227\n",
      "Transformer - Last Time Step MSE: 0.0527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer_results = transformer_model.evaluate(X_valid_seq, Y_valid_seq, verbose=1)\n",
    "print(f\"Transformer - Total MSE: {transformer_results[0]:.4f}\")\n",
    "print(f\"Transformer - Last Time Step MSE: {transformer_results[1]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797a529",
   "metadata": {},
   "source": [
    "#  Perbandingan Model Deep Learning untuk Time Series Forecasting\n",
    "\n",
    "Membandingkan berbagai arsitektur deep learning dalam tugas peramalan (forecasting) data time series. Dataset yang digunakan adalah data sintetis berupa kombinasi dua gelombang sinus dengan noise acak. Tujuan utamanya adalah memprediksi nilai masa depan dari urutan data historis. Evaluasi dilakukan menggunakan metrik **Mean Squared Error (MSE)** pada data validasi.\n",
    "\n",
    "---\n",
    "\n",
    "##  Ringkasan Eksekusi dan Performa Model\n",
    "\n",
    "###  Baseline Models\n",
    "- **Naive Model**: Menggunakan nilai terakhir dari input sebagai prediksi. *MSE tidak dihitung*.\n",
    "- **Linear Model**: Lapisan Dense sederhana.  \n",
    "  **MSE**: `0.0131`\n",
    "\n",
    "###  Simple & Deep RNN\n",
    "- **Simple RNN**: 1 lapisan SimpleRNN.  \n",
    "  **MSE**: `0.0038`\n",
    "- **Deep RNN**: 2 lapisan SimpleRNN.  \n",
    "  **MSE**: `0.0033`\n",
    "\n",
    "###  Multi-Step Forecasting (10 langkah ke depan)\n",
    "- **Seq-to-Vector (LSTM)**: Output berbentuk satu vektor.  \n",
    "  **MSE**: `0.0142`\n",
    "- **Seq-to-Seq (LSTM)**: Output berbentuk urutan.  \n",
    "  **Last Time Step MSE**: `0.0090`\n",
    "\n",
    "###  Advanced LSTM & GRU\n",
    "- **Advanced LSTM**: Dengan `BatchNormalization` & `Dropout`.  \n",
    "  **Last Time Step MSE**: `0.0069`\n",
    "- **Advanced GRU**: Arsitektur serupa, lebih ringan.  \n",
    "  **Last Time Step MSE**: `0.0053`\n",
    "\n",
    "###  Modern Architectures\n",
    "- **WaveNet (CNN-based)**: Dilated 1D convolution.  \n",
    "  **Last Time Step MSE**: `0.0036`\n",
    "- **Transformer**: Implementasi sederhana berbasis self-attention.  \n",
    "  **Last Time Step MSE**: `0.0527`\n",
    "\n",
    "---\n",
    "\n",
    "##  Tabel Perbandingan Performa\n",
    "\n",
    "| Model                 | Tipe Prediksi         | MSE Validasi             |\n",
    "|----------------------|-----------------------|--------------------------|\n",
    "| Linear               | Single-Step (1 langkah) | 0.0131                   |\n",
    "| Simple RNN           | Single-Step            | 0.0038                   |\n",
    "| Deep RNN             | Single-Step            | 0.0033                   |\n",
    "| Seq-to-Vec (LSTM)    | Multi-Step (10 langkah) | 0.0142                  |\n",
    "| Seq-to-Seq (LSTM)    | Multi-Step             | 0.0090 (langkah ke-10)  |\n",
    "| Advanced LSTM        | Multi-Step             | 0.0069 (langkah ke-10)  |\n",
    "| Advanced GRU         | Multi-Step             | 0.0053 (langkah ke-10)  |\n",
    "| WaveNet (CNN)        | Multi-Step             | 0.0036 (langkah ke-10)  |\n",
    "| Transformer          | Multi-Step             | 0.0527 (langkah ke-10)  |\n",
    "\n",
    "---\n",
    "\n",
    "##  Kesimpulan Akhir\n",
    "\n",
    "-  **Model rekuren (RNN, LSTM, GRU)** secara konsisten unggul dibanding baseline linear sederhana.\n",
    "- **GRU outperform LSTM** pada dataset ini, dengan arsitektur lebih ringkas & efisien.\n",
    "- **WaveNet (CNN)** adalah pemenang, mampu mempelajari pola jangka panjang dan menghasilkan MSE terendah.\n",
    "-  **Transformer** tampil buruk — arsitektur kompleks tanpa tuning yang tepat tidak selalu unggul.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
